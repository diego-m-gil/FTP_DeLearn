# FTP_DeLearn Class Notes

Welcome to the **FTP_DeLearn** repository – a collection of notes, exercises, and supplementary materials for the Deep Learning course. This repository is organized by lecture week and covers topics such as Multi-Layer Perceptrons (MLPs), overfitting, regularisation, bias-variance tradeoff, and more.

## Table of Contents

- [Overview](#overview)
- [How to Use This Repository](#how-to-use-this-repository)
- [Week-by-Week Notes](#week-by-week-notes)
  - [Week 1: Introduction & Basics](#week-1-introduction--basics)
  - [Week 2: Learning and Optimisation](#week-2-learning-and-optimisation)
  - [Week 3: Overfitting, Regularisation & MLPs](#week-3-overfitting-regularisation--mlps)
  - [Additional Weeks](#additional-weeks)
- [Practical Exercises](#practical-exercises)
- [References](#references)
- [Contributing](#contributing)
- [License](#license)

## Overview

This repository is maintained as part of the Master of Science in Engineering course *Data Management* (FTP_DeLearn). It contains structured Markdown files for each week’s lectures and practical exercises. The content includes theoretical summaries, practical tips, code snippets, and diagrams, all designed to help you prepare for exams and reinforce your understanding of deep learning concepts.

## How to Use This Repository

- **Navigation:** Browse the folder structure or use the table of contents in each Markdown file to quickly access topics.
- **Version Control:** Each update is version-controlled via Git, ensuring that you always have access to the latest revisions of the notes.
- **Collaboration:** Feel free to contribute improvements or ask questions via GitHub issues or pull requests if you're working in a team environment.

## Week-by-Week Notes

### Week 1: Introduction & Basics
- Overview of course structure, introductory concepts in deep learning, and basic neural network terminology.

### Week 2: Learning and Optimisation
- Detailed notes on learning algorithms, gradient descent, cost functions (e.g., MSE, Cross-Entropy), and early experiments with logistic regression and initial MLPs.

### Week 3: Overfitting, Regularisation & MLPs
- **Topics Covered:**
  - Recap of MLPs and neural network fundamentals.
  - Overfitting and generalisation challenges.
  - Bias-Variance tradeoff and its implications.
  - Regularisation methods such as Weight Decay (L2), L1 Regularisation, Dropout, and Early Stopping.
- **Files:**  
  - `week3_notes.md` – Contains detailed notes, diagrams, and practical exercise instructions.

### Additional Weeks
- Future weeks will be added as new content is covered in lectures and practical sessions.

## Practical Exercises

Each week’s folder includes practical exercises and code examples (e.g., implementations in PyTorch) to help you apply the theory. Make sure to review these exercises as they provide valuable hands-on experience with deep learning techniques.

## References

- Course lecture slides and materials provided by Prof. Stefan Keller.
- Recommended readings and external resources:
  - [Deep Learning Book by Goodfellow et al.](https://www.deeplearningbook.org/)
  - [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)
  - Additional online resources and articles linked within individual notes.

## Contributing

If you have suggestions for improving these notes or additional resources to share, feel free to open an issue or submit a pull request. Contributions are welcome to enhance the learning experience for everyone in the course.

## License

This repository is provided under the [MIT License](LICENSE). Feel free to use, modify, and distribute the content for educational purposes.

---

*Happy learning and best of luck in your deep learning journey!*
